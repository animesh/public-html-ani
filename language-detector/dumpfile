   [1]Ubuntu Forums > [2]The Ubuntu Forum Community > [3]Forum Archive >
   [4]Absolute Beginner Talk > [SOLVED] Get text from a website with wget?

   --------------------------------------------------------------------------

   [5]PDA

   View Full Version : [6][SOLVED] Get text from a website with wget?

   --------------------------------------------------------------------------

   Nerdriot
   March 16th, 2008, 01:01 AM
   Hi,

   This may be a stupid question, but is it possible to get just the text
   that's displayed on a website with wget, rather than the html, etc.?

   I'm trying to write a script that will get the automatically updated
   playlist of this radio station, and save it with a timestamp to a file.

   Thanks for any advice/help :)

   --------------------------------------------------------------------------

   JKeefe
   March 16th, 2008, 05:26 PM
   I don't know of a direct way to perform this task using wget. I would
   solve this problem by getting the HTML using wget, and then using sed and
   awk to extract the text that I need. This can get complicated pretty
   quickly, but here is an example...

   Suppose that I wanted to extract the listing of recently released shows
   from my favorite Internet TV network. First I grab the HTML using wget,
   and manually inspect the source. I look for some unique strings that I can
   use to delimit the information that I am trying to extract. In this case,
   the listing I want begins with "Just Released" and ends with "More New
   Releases".

   I'm going to use the following awk script to extract the text between the
   start and end delimiters.

   $ more strip.awk
   $0 ~ startPattern { active = 1 }
   $0 ~ endPattern { active = 0 }
   active == 1 { print $0 }

   In addition, I have a sed script that deletes all HTML tags, and another
   sed script that cleans up the formatting.

   $ more strip.sed
   # strip out HTML tags

   :begin

   /<[^>]*$/{
   N
   b begin
   }

   s/<[^>]*>//g

   $ more format.sed
   # remove leading whitespace
   s/^[ \t]*//

   # fix newlines
   s/.$//

   And here is how I put it all together...

   $ wget --output-document=rev3.html http://www.revision3.com/

   $ awk -f strip.awk startPattern="Just Released" endPattern="More New
   Releases" rev3.html | sed -f strip.sed -f format.sed > newShows.txt

   Learning the basics of sed and awk is well worth the effort. Once you have
   learned the basics, it becomes a breeze to create powerful scripts.

   --------------------------------------------------------------------------

   Nerdriot
   March 16th, 2008, 10:16 PM
   Thank you :)

   Actually, my issue has become increasingly difficult, as the text I was
   trying to get copied comes in a css form. So far, I've found no way of
   getting it...

   I did, however, find a pretty easy way of getting only text from a
   particular page, but not with wget. I used this, basically:

   lynx -dump www.website.com > dumpfile

   I have no idea why I didn't think of that sooner. It still doesn't get the
   text from the css, but it works with basic text, at least.

   About sed and awk; I've not gotten very familiar with awk yet (I should),
   but I've learned some basics with sed. I created a script to get videos
   from YouTube a while back, that uses wget -O, and some sed. Works nicely.
   Sed's gotta be one of my favorite commands ;)

   Anyway, thank you for taking the time to offer advice. I'll mark the
   thread as "solved" :)

   --------------------------------------------------------------------------

   vBulletin(R) v3.8.4, Copyright (c)2000-2010, Jelsoft Enterprises Ltd.

References

   Visible links
   1. http://ubuntuforums.org/archive/index.php
   2. http://ubuntuforums.org/archive/index.php/f-130.html
   3. http://ubuntuforums.org/archive/index.php/f-325.html
   4. http://ubuntuforums.org/archive/index.php/f-73.html
   5. http://ubuntuforums.org/archive/index.php/t-725749.html?pda=1
   6. http://ubuntuforums.org/showthread.php?t=725749
